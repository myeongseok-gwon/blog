{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsOnp1Y-TJy_"
   },
   "source": [
    "# Minimum Wage Example Notebook with DiD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In Progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trvqH1pjTJpR"
   },
   "source": [
    "This notebook implements Difference-in-Differences in an application on\n",
    "the effect of minimum wage changes on teen employment. We use data from\n",
    "[Callaway\n",
    "(2022)](https://bcallaway11.github.io/files/Callaway-Chapter-2022/main.pdf). The data are annual county level data from the United States covering 2001 to 2007. The outcome variable is log county-level teen employment, and the treatment variable is an indicator for whether the county has a minimum wage above the federal minimum wage. Note that this definition of the treatment variable makes the analysis straightforward but ignores the nuances of the exact value of the minimum wage in each county and how far those values are from the federal minimum. The data also include county population and county average annual pay.\n",
    "See [Callaway and Sant’Anna\n",
    "(2021)](https://www.sciencedirect.com/science/article/abs/pii/S0304407620303948)\n",
    "for additional details on the data.\n",
    "\n",
    "First, we will load some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIcmAX3IGG-6",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install doubleml~=0.7.0\n",
    "\n",
    "# hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MER7xs5U0DRL",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting formulaic\n",
      "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting interface-meta>=1.2.0 (from formulaic)\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from formulaic) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from formulaic) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from formulaic) (1.15.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from formulaic) (4.12.2)\n",
      "Collecting wrapt>=1.0 (from formulaic)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas>=1.0->formulaic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas>=1.0->formulaic) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas>=1.0->formulaic) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic) (1.17.0)\n",
      "Downloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
      "Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, interface-meta, formulaic\n",
      "Successfully installed formulaic-1.1.1 interface-meta-1.3.0 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install formulaic\n",
    "# hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting doubleml\n",
      "  Downloading DoubleML-0.9.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from doubleml) (1.4.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from doubleml) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from doubleml) (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from doubleml) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<1.6.0,>=1.4.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from doubleml) (1.5.2)\n",
      "Requirement already satisfied: statsmodels in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from doubleml) (0.14.4)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from doubleml) (3.10.1)\n",
      "Collecting plotly (from doubleml)\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from scikit-learn<1.6.0,>=1.4.0->doubleml) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->doubleml) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas->doubleml) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas->doubleml) (2025.1)\n",
      "Collecting narwhals>=1.15.1 (from plotly->doubleml)\n",
      "  Downloading narwhals-1.32.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from statsmodels->doubleml) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->doubleml) (1.17.0)\n",
      "Downloading DoubleML-0.9.3-py3-none-any.whl (342 kB)\n",
      "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-1.32.0-py3-none-any.whl (320 kB)\n",
      "Installing collected packages: narwhals, plotly, doubleml\n",
      "Successfully installed doubleml-0.9.3 narwhals-1.32.0 plotly-6.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install doubleml\n",
    "# hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFlG2QhXTJav",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import doubleml as dml\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RidgeCV, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(772023)\n",
    "# hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TXQ_GfxZXDeG",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from formulaic import Formula\n",
    "\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, formula):\n",
    "        self.formula = formula\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        return df.values.astype(np.float64)\n",
    "    \n",
    "    # hide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6jWjkrzU8I6"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "znh8YcAXSp3E"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/minwage_data.csv\",\n",
    "                   index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PQdsT6BnWKeq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>countyreal</th>\n",
       "      <th>state_name</th>\n",
       "      <th>year</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>emp0A01_BS</th>\n",
       "      <th>quarter</th>\n",
       "      <th>censusdiv</th>\n",
       "      <th>pop</th>\n",
       "      <th>annual_avg_pay</th>\n",
       "      <th>state_mw</th>\n",
       "      <th>fed_mw</th>\n",
       "      <th>treated</th>\n",
       "      <th>G</th>\n",
       "      <th>lemp</th>\n",
       "      <th>lpop</th>\n",
       "      <th>lavg_pay</th>\n",
       "      <th>region</th>\n",
       "      <th>ever_treated</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2001</td>\n",
       "      <td>2013</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2459</td>\n",
       "      <td>22155</td>\n",
       "      <td>5.65</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>7.807510</td>\n",
       "      <td>10.005818</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2002</td>\n",
       "      <td>2013</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2664</td>\n",
       "      <td>28447</td>\n",
       "      <td>5.65</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>7.887584</td>\n",
       "      <td>10.255798</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2003</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2715</td>\n",
       "      <td>30184</td>\n",
       "      <td>7.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>7.906547</td>\n",
       "      <td>10.315067</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2004</td>\n",
       "      <td>2013</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2677</td>\n",
       "      <td>27557</td>\n",
       "      <td>7.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.892452</td>\n",
       "      <td>10.224012</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2005</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2646</td>\n",
       "      <td>30396</td>\n",
       "      <td>7.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>7.880804</td>\n",
       "      <td>10.322066</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   countyreal state_name  year  FIPS  emp0A01_BS  quarter  censusdiv   pop  \\\n",
       "1        2013     Alaska  2001  2013          15        1          9  2459   \n",
       "2        2013     Alaska  2002  2013          17        1          9  2664   \n",
       "3        2013     Alaska  2003  2013          12        1          9  2715   \n",
       "4        2013     Alaska  2004  2013          13        1          9  2677   \n",
       "5        2013     Alaska  2005  2013          11        1          9  2646   \n",
       "\n",
       "   annual_avg_pay  state_mw  fed_mw  treated     G      lemp      lpop  \\\n",
       "1           22155      5.65    5.15        1  2001  2.708050  7.807510   \n",
       "2           28447      5.65    5.15        1  2001  2.833213  7.887584   \n",
       "3           30184      7.15    5.15        1  2001  2.484907  7.906547   \n",
       "4           27557      7.15    5.15        1  2001  2.564949  7.892452   \n",
       "5           30396      7.15    5.15        1  2001  2.397895  7.880804   \n",
       "\n",
       "    lavg_pay  region  ever_treated    id  \n",
       "1  10.005818       4             1  2013  \n",
       "2  10.255798       4             1  2013  \n",
       "3  10.315067       4             1  2013  \n",
       "4  10.224012       4             1  2013  \n",
       "5  10.322066       4             1  2013  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v37g7zlwW5pH"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We remove observations that are already treated in the first observed period (2001). We drop all variables that we won't use in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "W6ob7pptW49G"
   },
   "outputs": [],
   "source": [
    "data = data.loc[(data.G == 0) | (data.G > 2001)]\n",
    "data.drop(columns=[\"countyreal\", \"state_name\", \"FIPS\", \"emp0A01_BS\",\n",
    "                   \"quarter\", \"censusdiv\", \"pop\", \"annual_avg_pay\",\n",
    "                   \"state_mw\", \"fed_mw\", \"ever_treated\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri12EDNJaAfF"
   },
   "source": [
    "Next, we create the treatment groups. We focus our analysis exclusively on the set of counties that had wage increases away from the federal minimum wage in 2004. That is, we treat 2003 and earlier as the pre-treatment period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "huj7huQ1aQSq"
   },
   "outputs": [],
   "source": [
    "years = [2001, 2002, 2003, 2004, 2005, 2006, 2007]\n",
    "treat, cont = {}, {}\n",
    "for year in years:\n",
    "    treat[year] = data.loc[(data.G == 2004) & (data.year == year)].copy()\n",
    "    cont[year] = data.loc[((data.G == 0) | (data.G > year)) & (data.year == year)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HC1PX_Uc5bQ"
   },
   "source": [
    "We assume that the basic assumptions, particularly parallel trends, hold after conditioning on pre-treatment variables: 2001 population, 2001 average pay and 2001 teen employment, as well as the region in which the county is located. (The region is characterized by four\n",
    "categories.)\n",
    "\n",
    "Consequently, we want to extract the control variables for both treatment and control group in 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KvkwAdL6evsU"
   },
   "outputs": [],
   "source": [
    "treat[2001].drop(columns=[\"year\", \"G\", \"region\", \"treated\"], inplace=True)\n",
    "cont[2001].drop(columns=[\"year\", \"G\", \"region\", \"treated\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU7rM_5Ne3Xr"
   },
   "source": [
    "2003 serves as the pre-treatment period for both counties that do receive the treatment in 2004 and those that do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3cd3dBDqeyqa"
   },
   "outputs": [],
   "source": [
    "treatB = pd.merge(treat[2003], treat[2001], on=\"id\", suffixes=[\"_pre\", \"_0\"])\n",
    "treatB.drop(columns=[\"treated\", \"lpop_pre\", \"lavg_pay_pre\", \"year\", \"G\"], inplace=True)\n",
    "\n",
    "contB = pd.merge(cont[2003], cont[2001], on=\"id\", suffixes=[\"_pre\", \"_0\"])\n",
    "contB.drop(columns=[\"treated\", \"lpop_pre\", \"lavg_pay_pre\", \"year\", \"G\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL1fSfb5e82d"
   },
   "source": [
    "We estimate the ATET in 2004-2007, which corresponds to the effect in the year of treatment as well as in the three years after the treatment. The control observations are the observations that still have the federal minimum wage in each year. (The control group is shrinking in each year as additional units receive treatment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zvN6Nmy0gPy4"
   },
   "outputs": [],
   "source": [
    "tdid, cdid = {}, {}\n",
    "did_data = {}\n",
    "# the first year will be used for pre-trend testing later on\n",
    "for year in [2002, 2004, 2005, 2006, 2007]:\n",
    "    treat[year].drop(columns=[\"lpop\", \"lavg_pay\", \"year\", \"G\", \"region\"], inplace=True)\n",
    "    cont[year].drop(columns=[\"lpop\", \"lavg_pay\", \"year\", \"G\", \"region\"], inplace=True)\n",
    "\n",
    "    tdid[year] = pd.merge(treat[year], treatB, on=\"id\")\n",
    "    tdid[year][\"dy\"] = tdid[year][\"lemp\"] - tdid[year][\"lemp_pre\"]\n",
    "    tdid[year].drop(columns=[\"id\", \"lemp\", \"lemp_pre\"], inplace=True)\n",
    "    tdid[year].treated = 1  # forcing treatment to be 1, so that 2002 is \"treated\" when testing pre-trends\n",
    "\n",
    "    cdid[year] = pd.merge(cont[year], contB, on=\"id\")\n",
    "    cdid[year][\"dy\"] = cdid[year][\"lemp\"] - cdid[year][\"lemp_pre\"]\n",
    "    cdid[year].drop(columns=[\"id\", \"lemp\", \"lemp_pre\"], inplace=True)\n",
    "\n",
    "    # join control and treatment data and create one-hot-encoded region variables\n",
    "    did_data[year] = pd.concat((tdid[year], cdid[year]))\n",
    "    dummy_data = pd.get_dummies(did_data[year].region, drop_first=True, prefix=\"region\")\n",
    "    did_data[year] = pd.concat((did_data[year], dummy_data), axis=1).drop(columns=[\"region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RhFOwd40RQ6K"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treated</th>\n",
       "      <th>lemp_0</th>\n",
       "      <th>lpop_0</th>\n",
       "      <th>lavg_pay_0</th>\n",
       "      <th>dy</th>\n",
       "      <th>region_2</th>\n",
       "      <th>region_3</th>\n",
       "      <th>region_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.117994</td>\n",
       "      <td>9.776676</td>\n",
       "      <td>10.219356</td>\n",
       "      <td>-0.118482</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>10.677615</td>\n",
       "      <td>10.505150</td>\n",
       "      <td>0.065813</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7.334329</td>\n",
       "      <td>11.127204</td>\n",
       "      <td>10.162423</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3.737670</td>\n",
       "      <td>9.156940</td>\n",
       "      <td>10.283908</td>\n",
       "      <td>-0.336472</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>8.529912</td>\n",
       "      <td>9.837935</td>\n",
       "      <td>0.125163</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   treated    lemp_0     lpop_0  lavg_pay_0        dy  region_2  region_3  \\\n",
       "0        1  5.117994   9.776676   10.219356 -0.118482      True     False   \n",
       "1        1  6.302619  10.677615   10.505150  0.065813      True     False   \n",
       "2        1  7.334329  11.127204   10.162423  0.008202      True     False   \n",
       "3        1  3.737670   9.156940   10.283908 -0.336472      True     False   \n",
       "4        1  3.970292   8.529912    9.837935  0.125163      True     False   \n",
       "\n",
       "   region_4  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "did_data[2004].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqHmiHaZgPZz"
   },
   "source": [
    "### Estimation of the ATET with DML\n",
    "\n",
    "We estimate the ATET of the county level minimum wage being larger than the federal minimum with the DML algorithm presented in Section 16.3 in the book. This requires estimation of the nuisance functions $E[Y|D=0,X]$, $E[D|X]$ as well as $P(D = 1)$. For the conditional expectation functions, we will consider different modern ML regression methods, namely: Constant (= no controls); a linear combination of the controls; an expansion of the raw control variables including all third order interactions; Lasso (CV); Ridge (CV); Random Forest.\n",
    "The methods indicated with CV have their tuning parameter selected by cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WjxpBKUGG_C"
   },
   "source": [
    "The following code block implements the DML estimator with cross-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ut2Rrt-5GG_C"
   },
   "outputs": [],
   "source": [
    "def final_stage(D, y, Dhat, yhat, phat):\n",
    "    # doubly robust quantity for every sample\n",
    "    phihat = ((D - Dhat) / (phat * (1 - Dhat))) * (y - yhat)\n",
    "    point = np.mean(phihat) / np.mean(D / phat)\n",
    "    # influence function\n",
    "    phihat = (phihat - point * (D / phat)) / np.mean(D / phat)\n",
    "    var = np.mean(np.square(phihat))\n",
    "    stderr = np.sqrt(var / D.shape[0])\n",
    "    return point, stderr\n",
    "\n",
    "\n",
    "def dr_att(X, D, y, modely, modeld, *, trimming=0.01, nfolds=5):\n",
    "    '''\n",
    "    DML for the ATT estimation (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls, pandas DataFrame\n",
    "    D: treatment indicator, numpy array\n",
    "    y: the outcome (the delta before and after intervention in DiD), numpy array\n",
    "    modely: the ML model for predicting y (the delta Y_1 - Y_0 in DiD)\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    trimming: threshold below which to trim propensities, float (default=0.01)\n",
    "    nfolds: the number of folds in cross-fitting, int (default=5)\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment on the treated\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: cross-fitted predictions of the outcome y under control, E[y|D=0, X]\n",
    "    Dhat: cross-fitted predictions of the treatment D, E[D|X]\n",
    "    rmsey: the RMSE of the model y ~ X | D==0\n",
    "    rmseD: the RMSE of the model D ~ X\n",
    "    phat: the estimated treatment probability in each fold\n",
    "    '''\n",
    "    cv = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=1234)\n",
    "    # fit a model E[y | D=0, X]\n",
    "    yhat = np.zeros(y.shape)\n",
    "    for train, test in cv.split(X, D):\n",
    "        modely.fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "        yhat[test] = modely.predict(X.iloc[test])\n",
    "    # fit a model P[D]\n",
    "    phat = cross_val_predict(DummyRegressor(), X, D, cv=cv)\n",
    "    # propensity scores E[D | X]\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba')[:, 1]\n",
    "    # trimm propensity score\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "    # estimation of parameter\n",
    "    point, stderr = final_stage(D, y, Dhat, yhat, phat)\n",
    "    # nuisance function rmse's\n",
    "    rmsey = np.sqrt(np.mean((y - yhat)[D == 0]**2))\n",
    "    rmseD = np.sqrt(np.mean((D - Dhat)**2))\n",
    "    return point, stderr, yhat, Dhat, rmsey, rmseD, phat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6lBHCpuKXtu5"
   },
   "outputs": [],
   "source": [
    "def get_nuisance_learners(region_names):\n",
    "    ''' Constructs the learners we will consider for nuisance estimation\n",
    "\n",
    "    region_names: the available region names\n",
    "\n",
    "    Returns: a dictionary of learners of the form {key: (ml_g, ml_m)}\n",
    "    '''\n",
    "    # a formula that will be used within a featurizer to create interactions\n",
    "    # (region id) * (baseline outcome) + (region id) + (baseline outcomes)\n",
    "    formula = '0 + ' + ' + '.join([f\"{col} * (lemp_0 + lpop_0 + lavg_pay_0)\" for col in region_names])\n",
    "\n",
    "    # a generic polynomial featurizer\n",
    "    def poly():\n",
    "        return PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "    learners = {\n",
    "        # no X\n",
    "        \"No Controls\": (DummyRegressor(strategy=\"mean\"), DummyClassifier(strategy=\"prior\")),\n",
    "        # linear models of X\n",
    "        \"Basic\": (LinearRegression(), LogisticRegression(random_state=123)),\n",
    "        # linear models with region interactions\n",
    "        \"Expansion\": (make_pipeline(FormulaTransformer(formula), LinearRegression()),\n",
    "                      make_pipeline(FormulaTransformer(formula), LogisticRegression(random_state=123))),\n",
    "        # full blown third degree interactions of X, with Lasso\n",
    "        \"Lasso (CV)\": (make_pipeline(poly(), LassoCV(n_jobs=-1, random_state=123)),\n",
    "                       make_pipeline(poly(), LogisticRegressionCV(penalty=\"l1\", solver=\"liblinear\",\n",
    "                                                                  n_jobs=-1, random_state=123))),\n",
    "        # full blown third degree interactions, with Ridge\n",
    "        \"Ridge (CV)\": (make_pipeline(poly(), RidgeCV()),\n",
    "                       make_pipeline(poly(), LogisticRegressionCV(n_jobs=-1, random_state=123))),\n",
    "        # Random Forest regressors and classifiers\n",
    "        \"Random Forest\": (RandomForestRegressor(n_estimators=100, min_samples_leaf=20, max_features=4,\n",
    "                                                n_jobs=-1, random_state=123),\n",
    "                          RandomForestClassifier(n_estimators=100, min_samples_leaf=20, max_features=4,\n",
    "                                                 n_jobs=-1, random_state=123))\n",
    "    }\n",
    "    return learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQd7HP57GG_D"
   },
   "outputs": [],
   "source": [
    "def MinWageDiD(years):\n",
    "    # arrays for saving the results\n",
    "    att, se_att, RMSE_d, RMSE_y = {}, {}, {}, {}\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Estimating ATET for year {year}. Please wait.\")\n",
    "        att[year], se_att[year], RMSE_d[year], RMSE_y[year] = {}, {}, {}, {}\n",
    "\n",
    "        # set up the data for the specific year\n",
    "        X = did_data[year].drop(columns=[\"treated\", \"dy\"])  # controls\n",
    "        D = did_data[year].treated.values  # treatment\n",
    "        dy = did_data[year].dy.values  # delta outcome betwee post and pre treatment\n",
    "        region_names = [col for col in X.columns if col.startswith('region_')]\n",
    "\n",
    "        # get dictionary of nuisance learners\n",
    "        learners = get_nuisance_learners(region_names)\n",
    "\n",
    "        # for storing the nuisance predictions\n",
    "        pred_y, pred_d = {}, {}\n",
    "        for method, (ml_g, ml_m) in learners.items():\n",
    "            print(f\"Estimating {method}.\")\n",
    "            point, stderr, dyhat, Dhat, rmsey, rmsed, phat = dr_att(X, D, dy, ml_g, ml_m, trimming=0.01, nfolds=5)\n",
    "            pred_y[method], pred_d[method] = dyhat, Dhat\n",
    "            att[year][method], se_att[year][method] = point, stderr\n",
    "            RMSE_y[year][method], RMSE_d[year][method] = rmsey, rmsed\n",
    "\n",
    "        # find best model for dy and d and use predictions\n",
    "        besty = min(RMSE_y[year], key=RMSE_y[year].get)\n",
    "        bestd = min(RMSE_d[year], key=RMSE_d[year].get)\n",
    "        # phat this is the same for all methods and equal to the cross-fitted Pr[D=1]\n",
    "        point, stderr = final_stage(D, dy, pred_d[bestd], pred_y[besty], phat)\n",
    "        att[year][\"Best\"], se_att[year][\"Best\"] = point, stderr\n",
    "        RMSE_y[year][\"Best\"] = RMSE_y[year][besty]\n",
    "        RMSE_d[year][\"Best\"] = RMSE_d[year][bestd]\n",
    "\n",
    "        # we can also find the best linear combination of models via stacking\n",
    "        pred_y = pd.DataFrame(pred_y)\n",
    "        pred_d = pd.DataFrame(pred_d)\n",
    "        stacked_pred_y = LinearRegression().fit(pred_y, dy).predict(pred_y)\n",
    "        stacked_pred_d = LinearRegression().fit(pred_d, D).predict(pred_d)\n",
    "        # phat this is the same for all methods and equal to the cross-fitted Pr[D=1]\n",
    "        point, stderr = final_stage(D, dy, stacked_pred_d, stacked_pred_y, phat)\n",
    "        att[year][\"Stack\"], se_att[year][\"Stack\"] = point, stderr\n",
    "        RMSE_y[year][\"Stack\"] = np.sqrt(np.mean((dy - stacked_pred_y)[D == 0]**2))\n",
    "        RMSE_d[year][\"Stack\"] = np.sqrt(np.mean((D - stacked_pred_d)**2))\n",
    "\n",
    "    return att, se_att, RMSE_y, RMSE_d\n",
    "\n",
    "\n",
    "att, se_att, RMSE_y, RMSE_d = MinWageDiD([2004, 2005, 2006, 2007])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIwONJ0hGG_E"
   },
   "source": [
    "We start by reporting the RMSE obtained during cross-fitting for each learner in each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "592YvTt0GG_E"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">2004</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2005</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2006</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2007</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Controls</th>\n",
       "      <td>0.198170</td>\n",
       "      <td>0.163240</td>\n",
       "      <td>0.200574</td>\n",
       "      <td>0.188136</td>\n",
       "      <td>0.211096</td>\n",
       "      <td>0.223408</td>\n",
       "      <td>0.250283</td>\n",
       "      <td>0.230234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basic</th>\n",
       "      <td>0.190026</td>\n",
       "      <td>0.163042</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.185159</td>\n",
       "      <td>0.201083</td>\n",
       "      <td>0.217356</td>\n",
       "      <td>0.220808</td>\n",
       "      <td>0.221772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expansion</th>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.163512</td>\n",
       "      <td>0.192632</td>\n",
       "      <td>0.185682</td>\n",
       "      <td>0.201404</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.219538</td>\n",
       "      <td>0.221867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso (CV)</th>\n",
       "      <td>0.209429</td>\n",
       "      <td>0.163056</td>\n",
       "      <td>0.208909</td>\n",
       "      <td>0.185028</td>\n",
       "      <td>0.224792</td>\n",
       "      <td>0.217116</td>\n",
       "      <td>0.273589</td>\n",
       "      <td>0.221615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge (CV)</th>\n",
       "      <td>0.191350</td>\n",
       "      <td>0.162890</td>\n",
       "      <td>0.193674</td>\n",
       "      <td>0.185087</td>\n",
       "      <td>0.201878</td>\n",
       "      <td>0.217875</td>\n",
       "      <td>0.229623</td>\n",
       "      <td>0.221764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.190328</td>\n",
       "      <td>0.165177</td>\n",
       "      <td>0.192860</td>\n",
       "      <td>0.187938</td>\n",
       "      <td>0.200679</td>\n",
       "      <td>0.220051</td>\n",
       "      <td>0.224474</td>\n",
       "      <td>0.222787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best</th>\n",
       "      <td>0.190026</td>\n",
       "      <td>0.162890</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.185028</td>\n",
       "      <td>0.200679</td>\n",
       "      <td>0.217116</td>\n",
       "      <td>0.219538</td>\n",
       "      <td>0.221615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stack</th>\n",
       "      <td>0.189088</td>\n",
       "      <td>0.162180</td>\n",
       "      <td>0.191411</td>\n",
       "      <td>0.184427</td>\n",
       "      <td>0.199261</td>\n",
       "      <td>0.216870</td>\n",
       "      <td>0.216727</td>\n",
       "      <td>0.221134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   2004                2005                2006            \\\n",
       "                 RMSE D   RMSE dy    RMSE D   RMSE dy    RMSE D   RMSE dy   \n",
       "No Controls    0.198170  0.163240  0.200574  0.188136  0.211096  0.223408   \n",
       "Basic          0.190026  0.163042  0.192401  0.185159  0.201083  0.217356   \n",
       "Expansion      0.190206  0.163512  0.192632  0.185682  0.201404  0.218003   \n",
       "Lasso (CV)     0.209429  0.163056  0.208909  0.185028  0.224792  0.217116   \n",
       "Ridge (CV)     0.191350  0.162890  0.193674  0.185087  0.201878  0.217875   \n",
       "Random Forest  0.190328  0.165177  0.192860  0.187938  0.200679  0.220051   \n",
       "Best           0.190026  0.162890  0.192401  0.185028  0.200679  0.217116   \n",
       "Stack          0.189088  0.162180  0.191411  0.184427  0.199261  0.216870   \n",
       "\n",
       "                   2007            \n",
       "                 RMSE D   RMSE dy  \n",
       "No Controls    0.250283  0.230234  \n",
       "Basic          0.220808  0.221772  \n",
       "Expansion      0.219538  0.221867  \n",
       "Lasso (CV)     0.273589  0.221615  \n",
       "Ridge (CV)     0.229623  0.221764  \n",
       "Random Forest  0.224474  0.222787  \n",
       "Best           0.219538  0.221615  \n",
       "Stack          0.216727  0.221134  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1 = pd.concat({'RMSE dy': pd.DataFrame(RMSE_y),\n",
    "                    'RMSE D': pd.DataFrame(RMSE_d)}, axis=1)\n",
    "table1 = table1.swaplevel(0, 1, axis=1)\n",
    "table1 = table1.sort_index(axis=1)\n",
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sea-bOaoGG_F"
   },
   "source": [
    "It appears there is some signal in the regressors, as all methods outside of  LassoCV produce somewhat smaller RMSEs than the No Controls baseline. While it would be hard to reliably conclude which of the relatively good performing methods is statistically best here, Best (or a different ensemble) provides a good baseline that is principled in the sense that one could pre-commit to using the best learners without having first looked at the subsequent estimation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogTzW6JLGG_F"
   },
   "source": [
    "We report estimates of the ATET in each period in the following table, together with the aforementioned RMSE's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iPX5OnYdAb8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">2004</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2005</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2006</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2007</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>att</th>\n",
       "      <th>se</th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>att</th>\n",
       "      <th>se</th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>att</th>\n",
       "      <th>se</th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>att</th>\n",
       "      <th>se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Controls</th>\n",
       "      <td>0.198170</td>\n",
       "      <td>0.163240</td>\n",
       "      <td>-0.040018</td>\n",
       "      <td>0.019055</td>\n",
       "      <td>0.200574</td>\n",
       "      <td>0.188136</td>\n",
       "      <td>-0.076277</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.211096</td>\n",
       "      <td>0.223408</td>\n",
       "      <td>-0.116778</td>\n",
       "      <td>0.019763</td>\n",
       "      <td>0.250283</td>\n",
       "      <td>0.230234</td>\n",
       "      <td>-0.131077</td>\n",
       "      <td>0.022604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basic</th>\n",
       "      <td>0.190026</td>\n",
       "      <td>0.163042</td>\n",
       "      <td>-0.021253</td>\n",
       "      <td>0.020706</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.185159</td>\n",
       "      <td>-0.047500</td>\n",
       "      <td>0.020915</td>\n",
       "      <td>0.201083</td>\n",
       "      <td>0.217356</td>\n",
       "      <td>-0.052885</td>\n",
       "      <td>0.019828</td>\n",
       "      <td>0.220808</td>\n",
       "      <td>0.221772</td>\n",
       "      <td>-0.066983</td>\n",
       "      <td>0.023574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expansion</th>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.163512</td>\n",
       "      <td>-0.016902</td>\n",
       "      <td>0.023321</td>\n",
       "      <td>0.192632</td>\n",
       "      <td>0.185682</td>\n",
       "      <td>-0.049601</td>\n",
       "      <td>0.021917</td>\n",
       "      <td>0.201404</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>-0.052371</td>\n",
       "      <td>0.020507</td>\n",
       "      <td>0.219538</td>\n",
       "      <td>0.221867</td>\n",
       "      <td>-0.070848</td>\n",
       "      <td>0.025854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso (CV)</th>\n",
       "      <td>0.209429</td>\n",
       "      <td>0.163056</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>0.025408</td>\n",
       "      <td>0.208909</td>\n",
       "      <td>0.185028</td>\n",
       "      <td>-0.035780</td>\n",
       "      <td>0.025411</td>\n",
       "      <td>0.224792</td>\n",
       "      <td>0.217116</td>\n",
       "      <td>-0.043345</td>\n",
       "      <td>0.029495</td>\n",
       "      <td>0.273589</td>\n",
       "      <td>0.221615</td>\n",
       "      <td>-0.061821</td>\n",
       "      <td>0.030990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge (CV)</th>\n",
       "      <td>0.191350</td>\n",
       "      <td>0.162890</td>\n",
       "      <td>-0.023352</td>\n",
       "      <td>0.019436</td>\n",
       "      <td>0.193674</td>\n",
       "      <td>0.185087</td>\n",
       "      <td>-0.045136</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.201878</td>\n",
       "      <td>0.217875</td>\n",
       "      <td>-0.050672</td>\n",
       "      <td>0.019892</td>\n",
       "      <td>0.229623</td>\n",
       "      <td>0.221764</td>\n",
       "      <td>-0.063879</td>\n",
       "      <td>0.022801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.190328</td>\n",
       "      <td>0.165177</td>\n",
       "      <td>-0.023017</td>\n",
       "      <td>0.019238</td>\n",
       "      <td>0.192860</td>\n",
       "      <td>0.187938</td>\n",
       "      <td>-0.049144</td>\n",
       "      <td>0.020302</td>\n",
       "      <td>0.200679</td>\n",
       "      <td>0.220051</td>\n",
       "      <td>-0.050563</td>\n",
       "      <td>0.020030</td>\n",
       "      <td>0.224474</td>\n",
       "      <td>0.222787</td>\n",
       "      <td>-0.067078</td>\n",
       "      <td>0.023048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best</th>\n",
       "      <td>0.190026</td>\n",
       "      <td>0.162890</td>\n",
       "      <td>-0.018605</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.185028</td>\n",
       "      <td>-0.048510</td>\n",
       "      <td>0.021019</td>\n",
       "      <td>0.200679</td>\n",
       "      <td>0.217116</td>\n",
       "      <td>-0.048563</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>0.219538</td>\n",
       "      <td>0.221615</td>\n",
       "      <td>-0.076832</td>\n",
       "      <td>0.027336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stack</th>\n",
       "      <td>0.189088</td>\n",
       "      <td>0.162180</td>\n",
       "      <td>-0.021267</td>\n",
       "      <td>0.020405</td>\n",
       "      <td>0.191411</td>\n",
       "      <td>0.184427</td>\n",
       "      <td>-0.048563</td>\n",
       "      <td>0.020773</td>\n",
       "      <td>0.199261</td>\n",
       "      <td>0.216870</td>\n",
       "      <td>-0.049963</td>\n",
       "      <td>0.019983</td>\n",
       "      <td>0.216727</td>\n",
       "      <td>0.221134</td>\n",
       "      <td>-0.064935</td>\n",
       "      <td>0.024966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   2004                                    2005            \\\n",
       "                 RMSE D   RMSE dy       att        se    RMSE D   RMSE dy   \n",
       "No Controls    0.198170  0.163240 -0.040018  0.019055  0.200574  0.188136   \n",
       "Basic          0.190026  0.163042 -0.021253  0.020706  0.192401  0.185159   \n",
       "Expansion      0.190206  0.163512 -0.016902  0.023321  0.192632  0.185682   \n",
       "Lasso (CV)     0.209429  0.163056 -0.038656  0.025408  0.208909  0.185028   \n",
       "Ridge (CV)     0.191350  0.162890 -0.023352  0.019436  0.193674  0.185087   \n",
       "Random Forest  0.190328  0.165177 -0.023017  0.019238  0.192860  0.187938   \n",
       "Best           0.190026  0.162890 -0.018605  0.021212  0.192401  0.185028   \n",
       "Stack          0.189088  0.162180 -0.021267  0.020405  0.191411  0.184427   \n",
       "\n",
       "                                       2006                                \\\n",
       "                    att        se    RMSE D   RMSE dy       att        se   \n",
       "No Controls   -0.076277  0.020163  0.211096  0.223408 -0.116778  0.019763   \n",
       "Basic         -0.047500  0.020915  0.201083  0.217356 -0.052885  0.019828   \n",
       "Expansion     -0.049601  0.021917  0.201404  0.218003 -0.052371  0.020507   \n",
       "Lasso (CV)    -0.035780  0.025411  0.224792  0.217116 -0.043345  0.029495   \n",
       "Ridge (CV)    -0.045136  0.020036  0.201878  0.217875 -0.050672  0.019892   \n",
       "Random Forest -0.049144  0.020302  0.200679  0.220051 -0.050563  0.020030   \n",
       "Best          -0.048510  0.021019  0.200679  0.217116 -0.048563  0.020316   \n",
       "Stack         -0.048563  0.020773  0.199261  0.216870 -0.049963  0.019983   \n",
       "\n",
       "                   2007                                \n",
       "                 RMSE D   RMSE dy       att        se  \n",
       "No Controls    0.250283  0.230234 -0.131077  0.022604  \n",
       "Basic          0.220808  0.221772 -0.066983  0.023574  \n",
       "Expansion      0.219538  0.221867 -0.070848  0.025854  \n",
       "Lasso (CV)     0.273589  0.221615 -0.061821  0.030990  \n",
       "Ridge (CV)     0.229623  0.221764 -0.063879  0.022801  \n",
       "Random Forest  0.224474  0.222787 -0.067078  0.023048  \n",
       "Best           0.219538  0.221615 -0.076832  0.027336  \n",
       "Stack          0.216727  0.221134 -0.064935  0.024966  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = pd.concat({'att': pd.DataFrame(att),\n",
    "                    'se': pd.DataFrame(se_att),\n",
    "                    'RMSE dy': pd.DataFrame(RMSE_y),\n",
    "                    'RMSE D': pd.DataFrame(RMSE_d)}, axis=1)\n",
    "table2 = table2.swaplevel(0, 1, axis=1)\n",
    "table2 = table2.sort_index(axis=1)\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b42a0MLLGG_G"
   },
   "source": [
    "Here, we see that most methods provide point estimates that suggest the effect of the minimum wage increase leads to decreases in youth employment with small effects in the initial period that become larger in the years following the treatment. This pattern seems economically plausible as it may take time for firms to adjust employment and other input choices in response to a minimum wage change. In the estimates that are reported in the book we have values that are not consistent with this pattern, however, they systematically underperform in terms of having poor cross-fit prediction performance. In terms of point estimates, the other pattern that emerges is that all estimates that use the covariates produce ATET estimates that are systematically smaller in magnitude than the No Controls baseline, suggesting that failing to include the controls may lead to overstatement of treatment effects in this example.\n",
    "\n",
    "Turning to inference, we would reject the hypothesis of no minimum wage effect two or more years after the change at the 5% level, even after multiple testing correction, if we were to focus on many of the estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKKUIT8SGG_G"
   },
   "source": [
    "### Assess pre-trends\n",
    "\n",
    "Because we have data for the period 2001-2007, we can perform a so-called pre-trends test to provide some evidence about the plausibility of the conditional parallel trends assumption. Specifically, we can continue to use 2003 as the reference period but now consider 2002 to be the treatment period. Sensible economic mechanisms underlying the assumption would then typically suggest that the ATET in 2002 - before the 2004 minimum wage change we are considering - should be zero. Finding evidence that the ATET in 2002 is non-zero then calls into question the validity of the assumption.\n",
    "\n",
    "We change the treatment status of those observations, which received treatment in 2004 in the 2002 data and create a placebo treatment as well as control group. We call the same estimation pipeline that we used to obtain our ATET estimates for 2004-2007. We now hope that we get a statistical null result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dOw9mzvEIBZ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating ATET for year 2002. Please wait.\n",
      "Estimating No Controls.\n",
      "Estimating Basic.\n",
      "Estimating Expansion.\n",
      "Estimating Lasso (CV).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Ridge (CV).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Random Forest.\n"
     ]
    }
   ],
   "source": [
    "att, se_att, RMSE_y, RMSE_d = MinWageDiD([2002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NXBR0GnVJEKz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">2002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>RMSE D</th>\n",
       "      <th>RMSE dy</th>\n",
       "      <th>att</th>\n",
       "      <th>se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Controls</th>\n",
       "      <td>0.194396</td>\n",
       "      <td>0.154471</td>\n",
       "      <td>-0.004753</td>\n",
       "      <td>0.013299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basic</th>\n",
       "      <td>0.187976</td>\n",
       "      <td>0.154111</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.014207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expansion</th>\n",
       "      <td>0.188287</td>\n",
       "      <td>0.154192</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.014722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso (CV)</th>\n",
       "      <td>0.206890</td>\n",
       "      <td>0.154542</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.021796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge (CV)</th>\n",
       "      <td>0.188671</td>\n",
       "      <td>0.154331</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.013137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.191167</td>\n",
       "      <td>0.154875</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.013041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best</th>\n",
       "      <td>0.187976</td>\n",
       "      <td>0.154111</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.014207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stack</th>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.153370</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.013319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   2002                              \n",
       "                 RMSE D   RMSE dy       att        se\n",
       "No Controls    0.194396  0.154471 -0.004753  0.013299\n",
       "Basic          0.187976  0.154111  0.002826  0.014207\n",
       "Expansion      0.188287  0.154192  0.002756  0.014722\n",
       "Lasso (CV)     0.206890  0.154542  0.009917  0.021796\n",
       "Ridge (CV)     0.188671  0.154331  0.002240  0.013137\n",
       "Random Forest  0.191167  0.154875  0.001544  0.013041\n",
       "Best           0.187976  0.154111  0.002826  0.014207\n",
       "Stack          0.187600  0.153370  0.003229  0.013319"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2002 = pd.concat({'att': pd.DataFrame(att),\n",
    "                       'se': pd.DataFrame(se_att),\n",
    "                       'RMSE dy': pd.DataFrame(RMSE_y),\n",
    "                       'RMSE D': pd.DataFrame(RMSE_d)}, axis=1)\n",
    "table2002 = table2002.swaplevel(0, 1, axis=1)\n",
    "table2002 = table2002.sort_index(axis=1)\n",
    "table2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up95SCKAGG_J"
   },
   "source": [
    "Here we see broad agreement across all methods in the sense of returning point estimates that are small in magnitude and small relative to standard errors. In no case would we reject the hypothesis that the pre-event effect in 2002 is different from zero at usual levels of significance. We note that failing to reject the hypothesis of no pre-event effects certainly does not imply that the conditional DiD assumption is in fact satisfied. For example, confidence intervals include values that would be consistent with relatively large pre-event effects. However, it is reassuring to see that there is not strong evidence of a violation of the underlying identifying assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj0RZ1uu5EDO"
   },
   "source": [
    "### Visualizing the Effect of Treatment in 2004 Over the Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZGoHLiTa5C3H"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "table_all = pd.concat([table2002, table2], axis=1)\n",
    "years = np.array([2002, 2004, 2005, 2006, 2007])\n",
    "for it, method in enumerate(['No Controls', 'Best', 'Stack']):\n",
    "    plt.errorbar(years + (it - 1) * .1,\n",
    "                 np.array([table_all[year]['att'].loc[method] for year in years]),\n",
    "                 yerr=1.96 * np.array([table_all[year]['se'].loc[method] for year in years]),\n",
    "                 fmt='o', label=method)\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('ATET')\n",
    "plt.title('Effect of Treatment in 2004 Over the Years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqh57pA0GG_J"
   },
   "source": [
    "### Repication with `DoubleML`\n",
    "The high-level implementation `DoubleML` can be used to replicate the above. Please note, that it will run for a while (around 15min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jX6O2JB4PB2i"
   },
   "outputs": [],
   "source": [
    "def MinWageDiDwithDoubleML(years):\n",
    "    # arrays for saving the results\n",
    "    att, se_att, RMSE_d, RMSE_y = {}, {}, {}, {}\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Estimating ATET for year {year}. Please wait.\")\n",
    "        att[year], se_att[year], RMSE_d[year], RMSE_y[year] = {}, {}, {}, {}\n",
    "\n",
    "        # set up the data for the specific year\n",
    "        region_names = [col for col in did_data[year].columns if col.startswith(\"region_\")]\n",
    "        dml_data = dml.DoubleMLData(data=did_data[year],\n",
    "                                    x_cols=[\"lemp_0\", \"lpop_0\", \"lavg_pay_0\"] + region_names,\n",
    "                                    y_col=\"dy\",\n",
    "                                    d_cols=\"treated\")\n",
    "\n",
    "        # get dictionary of nuisance learners\n",
    "        learners = get_nuisance_learners(region_names)\n",
    "\n",
    "        for method, (ml_g, ml_m) in learners.items():\n",
    "            print(f\"Estimating {method}.\")\n",
    "            if method == \"Expansion\":\n",
    "                # we can't use the FormulaTransformer pipeline with DoubleML because\n",
    "                # it strips internally the pandas metadata\n",
    "                # so we have to explicitly transform the data before passing to DoubleML\n",
    "                trans_dml_data = dml.DoubleMLData.from_arrays(x=ml_g[0].fit_transform(did_data[year]),\n",
    "                                                              y=did_data[year].dy.values,\n",
    "                                                              d=did_data[year].treated.values)\n",
    "                dml_obj = dml.DoubleMLDID(trans_dml_data, ml_g=ml_g[1], ml_m=ml_m[1],\n",
    "                                          in_sample_normalization=False, trimming_threshold=0.01)\n",
    "            else:\n",
    "                dml_obj = dml.DoubleMLDID(dml_data, ml_g=ml_g, ml_m=ml_m,\n",
    "                                          in_sample_normalization=False, trimming_threshold=0.01)\n",
    "            # The following two lines are only needed to exactly replicate the custom\n",
    "            # implementation results, so that the same k-folds are used in cross-fitting\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "            dml_obj.set_sample_splitting(list(cv.split(did_data[year][dml_data.x_cols],\n",
    "                                                       did_data[year].treated.values)))\n",
    "            dml_obj.fit()\n",
    "            att[year][method] = dml_obj._coef[0]\n",
    "            se_att[year][method] = dml_obj._se[0]\n",
    "            RMSE_y[year][method] = np.mean(dml_obj.rmses[\"ml_g0\"])\n",
    "            RMSE_d[year][method] = np.mean(dml_obj.rmses[\"ml_m\"])\n",
    "\n",
    "    return att, se_att, RMSE_y, RMSE_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uVBmKdMkbSmW"
   },
   "outputs": [],
   "source": [
    "att, se_att, RMSE_y, RMSE_d = MinWageDiDwithDoubleML([2004, 2005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6blwjZM9Xt2f"
   },
   "outputs": [],
   "source": [
    "tabledml = pd.concat({'att': pd.DataFrame(att),\n",
    "                      'se': pd.DataFrame(se_att),\n",
    "                      'RMSE dy': pd.DataFrame(RMSE_y),\n",
    "                      'RMSE D': pd.DataFrame(RMSE_d)}, axis=1)\n",
    "tabledml = tabledml.swaplevel(0, 1, axis=1)\n",
    "tabledml = tabledml.sort_index(axis=1)\n",
    "tabledml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1BRGg3neyka"
   },
   "source": [
    "We find that the DoubleML package returns basically the same results as the custom implementation (presented below for comparison), modulo the small randomness due to the random sample splitting involved in the cross-fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bN1pGRFIe3Gc"
   },
   "outputs": [],
   "source": [
    "table2"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
